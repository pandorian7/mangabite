{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnb6XwW0nKEU"
      },
      "outputs": [],
      "source": [
        "#@title init\n",
        "\n",
        "blackurls = ['https://www.asurascans.com/wp-content/uploads/2021/05/ending-page-1.png']\n",
        "\n",
        "try:\n",
        "  import cloudscraper\n",
        "except:\n",
        "  !pip3 install cloudscraper\n",
        "  import cloudscraper\n",
        "\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from itertools import chain\n",
        "from PIL import ImageFile\n",
        "# from requests import get\n",
        "from bs4 import element\n",
        "from PIL import Image\n",
        "import io\n",
        "import shutil\n",
        "import http.client\n",
        "from collections import namedtuple\n",
        "from IPython.display import clear_output\n",
        "import requests\n",
        "\n",
        "get_res = namedtuple('GetRes', ['content', 'status_code'])\n",
        "\n",
        "import os\n",
        "\n",
        "def get(*args, **kwargs):\n",
        "    scraper = cloudscraper.create_scraper()\n",
        "    return scraper.get(*args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "from http.cookies import SimpleCookie\n",
        "rawdata ='wpmanga-reading-history=W3siaWQiOjY4LCJjIjoiMTAzNTIiLCJwIjoxLCJpIjoiIiwidCI6MTY2MjM2NjYwNH1d; __gpi=UID=0000097b7100ef5f:T=1662381056:RT=1662381056:S=ALNI_MYJnHzc0qqoRrzbl-_q3HT6tTmkow; _pbjs_userid_consent_data=3524755945110770; __gads=ID=e7d4017eb68ecc6d:T=1662381056:S=ALNI_MZh0n8IUw2dfYXUDjqs5-srpfsLVw; FCNEC=[[\"AKsRol8UoVBQAhqwOLTR5zQ0dHojTAxZ-D3J8bnBxntdCYZXNXls79EKWoEl9J2I2Xt50-lxhI6kjX3DkJ2nSnx_bK7FdN1qi8INbaA1iFg2CJIF68MWcQQ1g4EpGKN1N5RJJtHH3_o0XQYL-ZLdDsP7BijfkuWQow==\"],null,[]]; _cc_id=a7a75da27ba3c7d2449890ab9e6697a8; cto_bundle=PyWgil9wTGpkOUM1UTVaNmxtT3hKR05JNzBnJTJCOWl4VWp3RnVNMFpqT0p3NElEQVM3QVp3N1BsaEVVeWFVOWozMjNQJTJCMmlTSHBoRFp2Qm5qcEFRdERQbGVUOGZMMWtVeHA4JTJCVEtBJTJCZUNVcGVDRCUyQlpWNmc4d2NZNlI0SGUwaVA4clp2NGN1eE9Hd3RIOVJDT1VnN0Ywc1hpaDVBJTNEJTNE; _ga_X600HD8CLR=GS1.1.1662385321.2.0.1662385321.0.0.0; _gid=GA1.2.1768630006.1663112839; __cf_bm=fO0xPbtNcTeecz8pH8VE5luOUD3PRqw1DoVOT.qvOzo-1663112894-0-ASgjewvb/+79BRHuMJYwpDuFBrKnT2ojzqGBtAouqfdroPC1odqWGuvsYS0aCHqIHrRekIhjd0+HNBDpZiYzF08Qs+wNAaSWdzjGUPm5+7XpxWU6A+dCVWiC7GmxIIn/7w==; _ga=GA1.1.865995110.1662381032; _ga_GPHF8WRS7G=GS1.1.1663112838.3.1.1663113158.0.0.0; PHPSESSID=fgj2dnsfu3dl10g25805k72s2u'\n",
        "cookie = SimpleCookie()\n",
        "cookie.load(rawdata)\n",
        "cookies = {k: v.value for k, v in cookie.items()}\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "manga_root = '/content/drive/MyDrive/manga'\n",
        "addtional_paths = ['/content/drive/Shareddrives/TestDrive/manga']\n",
        "\n",
        "def Content(url, retries=10):\n",
        "  atempts = 0\n",
        "  while atempts < retries:\n",
        "    res = get(url)\n",
        "    if res.status_code == 200:\n",
        "      break\n",
        "    else:\n",
        "      if url in blackurls:break\n",
        "      print(f'{url} returened code {res.status_code}')\n",
        "    atempts += 1\n",
        "    \n",
        "  if atempts >= retries:\n",
        "    raise Exception(f'too many retries on {url}')\n",
        "  return res.content\n",
        "\n",
        "def Soup(url):\n",
        "  content = Content(url)\n",
        "  soup = BeautifulSoup(content)\n",
        "  return soup\n",
        "\n",
        "def Title(soup):\n",
        "  return soup.find('title').contents[0]\n",
        "\n",
        "def Values(soup, search_args, init_filter=None):\n",
        "  # first is the filter args for bs4.BS.find_all\n",
        "  # last is tag atribute\n",
        "  # thigs in between is for navigating the img tag\n",
        "  if init_filter:\n",
        "    soup = soup.find(*init_filter)\n",
        "  # print(soup)\n",
        "  root_eles = soup.find_all(*search_args[0])\n",
        "  eles = []\n",
        "  for ele in root_eles: \n",
        "    _ = ele\n",
        "    for q in search_args[1:-1]:\n",
        "      _ = _.find(*q)\n",
        "    eles.append(_)\n",
        "  if search_args[-1] is not None:\n",
        "    urls = [ele[search_args[-1]].strip() for ele in eles]\n",
        "  else:\n",
        "    urls = [ele.contents for ele in eles]\n",
        "  return urls\n",
        "\n",
        "def series_folder(title, root=manga_root):\n",
        "  directory = os.path.join(root, title)\n",
        "  try:\n",
        "    os.makedirs(directory)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  return directory\n",
        "\n",
        "def pdf_data(title, root='.'):\n",
        "  name = f\"{title}.pdf\"\n",
        "  path = os.path.join(root, name)\n",
        "  return name, path\n",
        "\n",
        "def save_pdf(pdf_path, image_urls):\n",
        "  interupted = False\n",
        "  images = [Image.open(io.BytesIO(Content(url))) for url in image_urls]\n",
        "  RGB_images = [img.convert('RGB') for img in images]\n",
        "  try:\n",
        "    RGB_images[0].save(pdf_path, save_all=True, append_images=RGB_images[1:])\n",
        "  except KeyboardInterrupt:\n",
        "    interupted = True\n",
        "  if interupted:\n",
        "    raise KeyboardInterrupt('raised deled interuption')\n",
        "\n",
        "def Domain(url):\n",
        "  return urlparse(url).netloc\n",
        "\n",
        "\n",
        "\n",
        "class AnimeSite:\n",
        "  def __init__(self, domain, **kwargs):\n",
        "    self.domain = Domain(domain)\n",
        "    for key, value in kwargs.items():\n",
        "      setattr(self, key, value)\n",
        "\n",
        "\n",
        "# replace terms:\n",
        "RS = (' - Reaper Scans','') #reaperscans.com\n",
        "MS1 = (' - Manga - Manhwa Read Free!', '') #mangastic.me\n",
        "MS2 = (' – Manga – Manhwa Read Free!', '')\n",
        "\n",
        "# search_terms:\n",
        "RS_ch = [['img'],'src']\n",
        "RS_series_old = [['div',{'class':'chapter-link'}], ['a'], 'href']\n",
        "RS_series=[ ['li'],['a'],'href']\n",
        "MS_ch = RS_ch\n",
        "MS_series = [['li',{'class':'wp-manga-chapter '}], ['a'], 'href']\n",
        "\n",
        "def MS_breadcrumb(soup):\n",
        "  breadcrumb = Values(soup.find('ol',{'class':'breadcrumb'}),[['li'],None])\n",
        "  breadcrumb = list(chain(*breadcrumb))\n",
        "  breadcrumb = list(filter(lambda e:e != '\\n', breadcrumb))\n",
        "  # if type(breadcrumb[-1]) == element.Tag:\n",
        "  #   series_name = breadcrumb[-1].contents[0].strip()\n",
        "  #   chapter = None\n",
        "  # else:\n",
        "  #   series_name = breadcrumb[-2].contents[0].strip()\n",
        "  #   chapter = breadcrumb[-1].strip()\n",
        "  return breadcrumb\n",
        "\n",
        "def MS_kind(soup):\n",
        "  breadcrumb = MS_breadcrumb(soup)\n",
        "  if type(breadcrumb[-1]) == element.Tag:\n",
        "    return 'series'\n",
        "  else:\n",
        "    return 'chapter'\n",
        "\n",
        "def RS_kind(soup):\n",
        "  navs = soup.find_all('nav')\n",
        "  if len(navs) > 1:\n",
        "    return 'chapter'\n",
        "  else:\n",
        "    return 'series'\n",
        "  print(navs)\n",
        "  try:\n",
        "    MS_kind(soup)\n",
        "    return 'chapter'\n",
        "  except:\n",
        "    return 'series'\n",
        "\n",
        "def MS_names(soup):\n",
        "  breadcrumb = MS_breadcrumb(soup)\n",
        "  series_name = breadcrumb[-2].contents[0].strip()\n",
        "  chapter_name = breadcrumb[-1].strip()\n",
        "  return series_name, chapter_name\n",
        "  \n",
        "def download_chapter(soup, site, hot_download=False):\n",
        "  series_name, chapter_name = site.names(soup)\n",
        "  pdf_root = series_folder(series_name)\n",
        "  pdf_name, pdf_path = pdf_data(f'{series_name} - {chapter_name}', root=pdf_root)\n",
        "  if os.path.exists(pdf_path):\n",
        "    print(f'{pdf_name} is already there!')\n",
        "    return\n",
        "  \n",
        "  image_urls = Values(soup, site.chapter, site.init)\n",
        "  print(image_urls)\n",
        "  image_urls = [i for i in image_urls if i not in blackurls]\n",
        "  print(pdf_name)\n",
        "  save_pdf(pdf_path, image_urls)\n",
        "  for add_path in addtional_paths:\n",
        "    dest = series_folder(series_name, add_path)\n",
        "    shutil.copy(pdf_path, dest)\n",
        "\n",
        "  if hot_download:\n",
        "    files.download(pdf_path)\n",
        "\n",
        "def AS_kind(soup):\n",
        "  n = len(soup.find('div',{'class':'ts-breadcrumb bixbox'}).find_all('li'))\n",
        "  if n == 2:\n",
        "    return 'series'\n",
        "  if n == 3:\n",
        "    return 'chapter'\n",
        "def AS_names(soup):\n",
        "  full_name = soup.find('h1',{\"class\":\"entry-title\"}).contents[0].strip()\n",
        "  series_name = soup.find('div',{\"class\",\"allc\"}).a.contents[0].strip()\n",
        "  chapter_name = full_name[len(series_name):].strip()\n",
        "  return series_name, chapter_name\n",
        "\n",
        "def mangadass_kind(soup):\n",
        "  n = len(soup.find(\"ol\", {\"class\":\"breadcrumb\"}).find_all(\"li\"))\n",
        "  if n == 2:\n",
        "    return 'series'\n",
        "  if n == 3:\n",
        "    return 'chapter'\n",
        "\n",
        "def mangadass_names(soup):\n",
        "  names = soup.find(\"ol\", {\"class\":\"breadcrumb\"}).find_all(\"li\")\n",
        "  series_name = names[1].find('a').contents[0].strip()\n",
        "  chapter_name = names[2].find('a').contents[0].strip()\n",
        "  return series_name, chapter_name\n",
        "\n",
        "def leviatanscans_kind(soup):\n",
        "  heading = soup.find_all('div', {'id':'manga-title'})\n",
        "  n = len(heading)\n",
        "  if n == 1:\n",
        "    return 'series'\n",
        "  else:\n",
        "    return 'chapter'\n",
        "\n",
        "def leviatanscans_names(soup):\n",
        "  breadcrumb = soup.find('ol', {'class':'breadcrumb'})\n",
        "  lis = breadcrumb.find_all('li')\n",
        "  series_name = lis[-2].a.contents[0].strip()\n",
        "  chapter_name = lis[-1].contents[0].strip()\n",
        "  return series_name, chapter_name\n",
        "\n",
        "AS_ch = [['img',{'loading':'lazy'}],'src']\n",
        "AS_series = [['div',{'class':'eph-num'}],['a'],'href']\n",
        "\n",
        "manga_sites = {\n",
        "    'mangastic.me': AnimeSite('https://mangastic.me/',\n",
        "                              replace=[MS1, MS2],\n",
        "                              chapter=MS_ch,\n",
        "                              series=MS_series,\n",
        "                              kind=MS_kind,\n",
        "                              names=MS_names,\n",
        "                              init=None\n",
        "                              ),\n",
        "    'reaperscans.com': AnimeSite('https://reaperscans.com/',\n",
        "                                 replace=[RS],\n",
        "                                 chapter=RS_ch,\n",
        "                                 series=RS_series,\n",
        "                                 kind=RS_kind,\n",
        "                                 names=MS_names,\n",
        "                                 init=['p',{'class':'py-4'}],\n",
        "                                 s_init=['ul',{'role':'list'}]\n",
        "                                 ),\n",
        "      'reincarnatedasthecrazedheir.com':AnimeSite('https://reincarnatedasthecrazedheir.com/'),\n",
        "      'www.asurascans.com':AnimeSite('https://www.asurascans.com/',\n",
        "                                     chapter=AS_ch,\n",
        "                                     series=AS_series,\n",
        "                                     kind=AS_kind,\n",
        "                                     names=AS_names,\n",
        "                                     init=['div',{'id':'readerarea'}]\n",
        "                                     ),\n",
        "      \"imperfectcomic.org\":AnimeSite('imperfectcomic.org',\n",
        "                                     chapter=[['img'],'src'],\n",
        "                                     series=AS_series,\n",
        "                                     kind=AS_kind,\n",
        "                                     names=AS_names,\n",
        "                                     init=['div',{'id':'readerarea'}]\n",
        "                                     ),\n",
        "      \"mangadass.com\":AnimeSite(\"https://mangadass.com/\",\n",
        "                                chapter=[['img'],'data-src'],\n",
        "                                series=[['li',{\"class\",\"a-h\"}], ['a'], \"href\"],\n",
        "                                kind=mangadass_kind,\n",
        "                                names=mangadass_names,\n",
        "                                init=['div',{\"class\":\"read-content\"}]\n",
        "                                ),\n",
        "      \"en.leviatanscans.com\":AnimeSite(\"https://en.leviatanscans.com/\",\n",
        "                                       chapter=[['img'], 'src'],\n",
        "                                       series=[['a'],'href'],\n",
        "                                       kind=leviatanscans_kind,\n",
        "                                       names=leviatanscans_names,\n",
        "                                       init=['div',{'class':'reading-content'}]),\n",
        "      \"mangahub.io\":AnimeSite(\"https://mangahub.io/\",\n",
        "                              chapter=[['img'], 'src'],\n",
        "                              init=['div',{'class':'_2aWyJ'}])\n",
        "}\n",
        "manga_sites['asura.gg']=manga_sites['www.asurascans.com']\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "bFWTLQSujbaR",
        "outputId": "45f17d18-140b-4c65-a9cd-199f32a68806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chapter\n",
            "dl\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b663fb354619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdownload_if_chapter_as_well\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b663fb354619>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(url, hot_download)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'failed:retrying...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b663fb354619>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(url, hot_download)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'chapter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mdownload_chapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhot_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7430ff743c2f>\u001b[0m in \u001b[0;36mdownload_chapter\u001b[0;34m(soup, site, hot_download)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0mimage_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0mimage_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_urls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblackurls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7430ff743c2f>\u001b[0m in \u001b[0;36mValues\u001b[0;34m(soup, search_args, init_filter)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0meles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msearch_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msearch_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7430ff743c2f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0meles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msearch_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msearch_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n\u001b[1;32m   1070\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'src'"
          ]
        }
      ],
      "source": [
        "url = \"https://asura.gg/reaper-of-the-drifting-moon-chapter-39/\" #@param {type:\"string\"}\n",
        "download_if_chapter_as_well = True #@param {type:\"boolean\"}\n",
        "def process(url,hot_download=False):\n",
        "  domain = Domain(url)\n",
        "  site = manga_sites[domain]\n",
        "  if not site:\n",
        "    raise Exception(f'no records of {domain}')\n",
        "  soup = Soup(url)\n",
        "  kind = site.kind(soup)\n",
        "  print(kind)\n",
        "\n",
        "  if kind == 'chapter':\n",
        "    while True:\n",
        "      try:\n",
        "        site = manga_sites.get(domain)\n",
        "        if not site:\n",
        "          raise Exception(f'no records of {domain}')\n",
        "        soup = Soup(url)\n",
        "        kind = site.kind(soup)\n",
        "        if kind == 'chapter':\n",
        "          print('dl')\n",
        "          download_chapter(soup, site, hot_download) \n",
        "        break\n",
        "      except Exception as err:\n",
        "        raise err\n",
        "        print('failed:retrying...')\n",
        "  if kind == 'series':\n",
        "    if domain == \"en.leviatanscans.com\":\n",
        "      soup = BeautifulSoup(requests.post(url+'/ajax/chapters').content)\n",
        "    chapter_links = Values(soup, site.series, getattr(site, 's_init', None))\n",
        "    print(chapter_links)\n",
        "    chapter_links.reverse()\n",
        "    for chapter in chapter_links:\n",
        "      if chapter[0] == '/':\n",
        "        chapter = \"https://\"+domain + chapter\n",
        "      process(chapter)\n",
        "\n",
        "process(url,download_if_chapter_as_well)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6BwqwpITi4j6"
      },
      "outputs": [],
      "source": [
        "#@title custom schemes\n",
        "custom_scheme = {\n",
        "    'reincarnatedasthecrazedheir.com': [['div', {'class':\"separator\"}],['img'],'src'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "wR-FPs8DizxR",
        "outputId": "17bcd24e-5166-425c-d3a8-3b922db62080"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d77c28fc9afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'no records of {domain}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpdf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: no records of zinmanga.com"
          ]
        }
      ],
      "source": [
        "#@title Custom Download\n",
        "url = \"https://zinmanga.com/manga/mercenary-enrollment/chapter-103/\" #@param {type:\"string\"}\n",
        "domain = Domain(url)\n",
        "site = custom_scheme.get(domain)\n",
        "if not site:\n",
        "  raise Exception(f'no records of {domain}')\n",
        "soup = Soup(url)\n",
        "pdf_name = Title(soup) + '.pdf'\n",
        "if os.path.exists(pdf_name):\n",
        "  os.remove(pdf_name)\n",
        "image_urls = Values(soup, site)\n",
        "save_pdf(pdf_name, image_urls)\n",
        "files.download(pdf_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77uSHOXhnBkJ"
      },
      "outputs": [],
      "source": [
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "path = [['img'], 'src']\n",
        "init = ['div',{'class':'_2aWyJ'}]\n",
        "\n",
        "def main():\n",
        "  url = \"https://mangahub.io/chapter/dr-stone_111/chapter-109\"\n",
        "  res = scraper.get(url)\n",
        "  print(res.status_code)\n",
        "  if not res.ok:return\n",
        "  soup = BeautifulSoup(res.content)\n",
        "  data = Values(soup, path, init)\n",
        "  print(data)\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrcH3loMocau"
      },
      "outputs": [],
      "source": [
        "scraper.get('https://img.mghubcdn.com/file/imghub/dr-stone/109/1.jpg')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1b77705ea340033c3a3e4e7b21d58951039657db9dbc46cbe94722876a4bd1bc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
